{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronaksharma/Development/resume_genie/resgen_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from config import OPENAI_API_KEY, AZURE_OPENAI_API_KEY,AZURE_API_ENDPOINT,API_VERSION,AZURE_MODEL_NAME, MODEL_NAME\n",
    "from openai import AzureOpenAI\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Azure OpenAI Client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY, \n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint = AZURE_API_ENDPOINT\n",
    "  )  \n",
    "\n",
    "# Helper Functions\n",
    "def get_embedding(client,text):\n",
    "    response = client.embeddings.create(\n",
    "        input = text,  \n",
    "        model=\"text-embedding-3-large\", # replace with small model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "  \n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    dot_product = sum(embedding1[i] * embedding2[i] for i in range(len(embedding1)))\n",
    "    magnitude1 = sum(x**2 for x in embedding1)**0.5\n",
    "    magnitude2 = sum(x**2 for x in embedding2)**0.5\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "  \n",
    "  \n",
    "def answer_question(client,persona, question, example=\"\"):\n",
    "  completion = client.chat.completions.create(\n",
    "    model=AZURE_MODEL_NAME,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": f\"{persona}\"},\n",
    "      {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "      {\"role\": \"assistant\", \"content\": f\" You can use the following information as an example: {example}\"},\n",
    "    ],\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def add_embedding_to_db(collection_of_resumes_db,embedding,text,id):\n",
    "    collection_of_resumes_db.add(\n",
    "        embeddings = [embedding],\n",
    "        documents = [text],\n",
    "        ids = [id]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1314 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m resume_raw_data_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mResume.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2TokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m resume_raw_data_df[\u001b[39m'\u001b[39m\u001b[39mnumber_tokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m resume_raw_data_df\u001b[39m.\u001b[39;49mResume_str\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39mlen\u001b[39;49m(tokenizer\u001b[39m.\u001b[39;49mencode(x)))\n\u001b[1;32m      5\u001b[0m resume_raw_data_df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m resume_raw_data_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mResume.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2TokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m resume_raw_data_df[\u001b[39m'\u001b[39m\u001b[39mnumber_tokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m resume_raw_data_df\u001b[39m.\u001b[39mResume_str\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39;49mencode(x)))\n\u001b[1;32m      5\u001b[0m resume_raw_data_df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2843\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2806\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2807\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2808\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2827\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m   2828\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2829\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2830\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[1;32m   2842\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2843\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2844\u001b[0m         text,\n\u001b[1;32m   2845\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2846\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2847\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2848\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2849\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2850\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2851\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2852\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2853\u001b[0m     )\n\u001b[1;32m   2855\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3255\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3245\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3246\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3247\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3248\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3253\u001b[0m )\n\u001b[0;32m-> 3255\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   3256\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   3257\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   3258\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3259\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3260\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3261\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3262\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3263\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3264\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3265\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3266\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3267\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3268\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3269\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3270\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3271\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3272\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3273\u001b[0m     split_special_tokens\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39msplit_special_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit_special_tokens),\n\u001b[1;32m   3274\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3275\u001b[0m )\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:137\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    133\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m )\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_encode_plus(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:601\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    579\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    580\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    599\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    600\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 601\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    602\u001b[0m         batched_input,\n\u001b[1;32m    603\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    604\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    605\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    606\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    607\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    608\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    609\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    610\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    611\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    612\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    613\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    614\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    615\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    616\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    617\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    618\u001b[0m         split_special_tokens\u001b[39m=\u001b[39;49msplit_special_tokens,\n\u001b[1;32m    619\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:127\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    123\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m )\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_batch_encode_plus(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Development/resume_genie/resgen_env/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:528\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mencode_special_tokens \u001b[39m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mencode_special_tokens \u001b[39m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 528\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    529\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    530\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    531\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    532\u001b[0m )\n\u001b[1;32m    534\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    540\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    542\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    552\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate the number of tokens\n",
    "resume_raw_data_df = pd.read_csv('Resume.csv')\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "resume_raw_data_df['number_tokens'] = resume_raw_data_df.Resume_str.apply(lambda x: len(tokenizer.encode(x)))\n",
    "resume_raw_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>number_tokens</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16852973</td>\n",
       "      <td>City  ,   State     Helps to develop policies...</td>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16852973</td>\n",
       "      <td>.         Advanced Medical Claims Analyst     ...</td>\n",
       "      <td>275</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16852973</td>\n",
       "      <td>ng and Advertising, working on public relation...</td>\n",
       "      <td>366</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16852973</td>\n",
       "      <td>ainte Genevieve Senior High   －   City  ,   St...</td>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str  number_tokens  \\\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...            318   \n",
       "1  16852973   City  ,   State     Helps to develop policies...            217   \n",
       "2  16852973  .         Advanced Medical Claims Analyst     ...            275   \n",
       "3  16852973  ng and Advertising, working on public relation...            366   \n",
       "4  16852973  ainte Genevieve Senior High   －   City  ,   St...            140   \n",
       "\n",
       "   index  \n",
       "0      1  \n",
       "1      2  \n",
       "2      3  \n",
       "3      4  \n",
       "4      5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunking data into roughly around 300 tokens\n",
    "\n",
    "# Using characters for chunking than tokens, as you can do list extraction based on character and not tokens\n",
    "resume_latest_data_df = pd.DataFrame(columns=[\"ID\", \"Resume_str\", \"number_tokens\"])\n",
    "\n",
    "# resume_raw_data_df['ID']\n",
    "max_tokens = 1200 # characters :  equivalent of 300 tokens \n",
    "counter = 0\n",
    "new_index=0\n",
    "\n",
    "for index, row in resume_raw_data_df.iterrows():\n",
    "    counter = 0\n",
    "    number_of_split = (row['number_tokens'])*4 // max_tokens # multiplying by 4 as each token is apporx 4 characters    \n",
    "    for i in range(number_of_split+2):\n",
    "        row_value = row['Resume_str'][counter: max_tokens+counter]\n",
    "        row_token =len(tokenizer.encode(row_value))\n",
    "        resume_latest_data_df.loc[new_index] = [resume_raw_data_df['ID'][index],row_value,row_token]\n",
    "        counter=max_tokens+counter\n",
    "        new_index+=1\n",
    "\n",
    "resume_latest_data_df[\"index\"] =[i for i in range(1, resume_latest_data_df.shape[0]+1)]\n",
    "resume_latest_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "# trial_Df = resume_latest_data_df.loc[resume_latest_data_df['ID'] == 18297650]\n",
    "# trial_Df\n",
    "# for index, row in trial_Df.iterrows():\n",
    "#     print(row['Resume_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Generate Embeddings\n",
    "resume_latest_data_df['embeddings']=resume_latest_data_df.Resume_str.apply(lambda x: get_embedding(azure_client,x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_latest_data_df.to_pickle('resume_data_pickle.pkl')    #to save the dataframe, df to 123.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>number_tokens</th>\n",
       "      <th>index</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.02123790979385376, -0.016999313607811928, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16852973</td>\n",
       "      <td>City  ,   State     Helps to develop policies...</td>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.006911890115588903, -0.021714137867093086,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16852973</td>\n",
       "      <td>.         Advanced Medical Claims Analyst     ...</td>\n",
       "      <td>275</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.028307681903243065, 0.041269928216934204, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>16852973</td>\n",
       "      <td>ng and Advertising, working on public relation...</td>\n",
       "      <td>366</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.020977875217795372, 0.0025092707946896553,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16852973</td>\n",
       "      <td>ainte Genevieve Senior High   －   City  ,   St...</td>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "      <td>[-0.009055311791598797, 0.016410766169428825, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID                                         Resume_str  \\\n",
       "0           0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1           1  16852973   City  ,   State     Helps to develop policies...   \n",
       "2           2  16852973  .         Advanced Medical Claims Analyst     ...   \n",
       "3           3  16852973  ng and Advertising, working on public relation...   \n",
       "4           4  16852973  ainte Genevieve Senior High   －   City  ,   St...   \n",
       "\n",
       "   number_tokens  index                                         embeddings  \n",
       "0            318      1  [-0.02123790979385376, -0.016999313607811928, ...  \n",
       "1            217      2  [-0.006911890115588903, -0.021714137867093086,...  \n",
       "2            275      3  [-0.028307681903243065, 0.041269928216934204, ...  \n",
       "3            366      4  [-0.020977875217795372, 0.0025092707946896553,...  \n",
       "4            140      5  [-0.009055311791598797, 0.016410766169428825, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRY STORING PICKLE AND THEN DATABASE\n",
    "df1 = pd.read_pickle('resume_data_pickle.pkl')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resume_latest_data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resume_latest_data_df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mresume_data_embeddings.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resume_latest_data_df' is not defined"
     ]
    }
   ],
   "source": [
    "resume_latest_data_df.to_csv(\"resume_latest_data_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_latest_data_df = pd.read_csv('resume_data_embeddings.csv') #convert string stored embeddings back to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "resume_latest_data_df = pd.read_csv('resume_data_embeddings.csv', converters={'embeddings': literal_eval}) #convert string stored embeddings back to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>number_tokens</th>\n",
       "      <th>index</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.02123790979385376, -0.016999313607811928, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16852973</td>\n",
       "      <td>City  ,   State     Helps to develop policies...</td>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.006911890115588903, -0.021714137867093086,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16852973</td>\n",
       "      <td>.         Advanced Medical Claims Analyst     ...</td>\n",
       "      <td>275</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.028307681903243065, 0.041269928216934204, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>16852973</td>\n",
       "      <td>ng and Advertising, working on public relation...</td>\n",
       "      <td>366</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.020977875217795372, 0.0025092707946896553,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16852973</td>\n",
       "      <td>ainte Genevieve Senior High   －   City  ,   St...</td>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "      <td>[-0.009055311791598797, 0.016410766169428825, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID                                         Resume_str  \\\n",
       "0           0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1           1  16852973   City  ,   State     Helps to develop policies...   \n",
       "2           2  16852973  .         Advanced Medical Claims Analyst     ...   \n",
       "3           3  16852973  ng and Advertising, working on public relation...   \n",
       "4           4  16852973  ainte Genevieve Senior High   －   City  ,   St...   \n",
       "\n",
       "   number_tokens  index                                         embeddings  \n",
       "0            318      1  [-0.02123790979385376, -0.016999313607811928, ...  \n",
       "1            217      2  [-0.006911890115588903, -0.021714137867093086,...  \n",
       "2            275      3  [-0.028307681903243065, 0.041269928216934204, ...  \n",
       "3            366      4  [-0.020977875217795372, 0.0025092707946896553,...  \n",
       "4            140      5  [-0.009055311791598797, 0.016410766169428825, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXTRA\n",
    "# len(resume_latest_data_df)\n",
    "resume_latest_data_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "15841\n"
     ]
    }
   ],
   "source": [
    "# Below 3 lines added for older python version\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"vector_database\",\n",
    "    settings=Settings(),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "collection_of_resumes_db = chroma_client.get_or_create_collection(name=\"resume_vector\")\n",
    "\n",
    "# student_info = \"\"\"\n",
    "# Alex, a 19-year-old computer science sophomore with a 3.7 GPA\n",
    "# \"\"\"\n",
    "# colour_info = \"\"\"\n",
    "# Alex's favourite color is Red\n",
    "# \"\"\"\n",
    "# embedding_1 = get_embedding(azure_client,student_info)\n",
    "\n",
    "# embedding_2 = get_embedding(azure_client,colour_info)\n",
    "\n",
    "# collection_of_resumes_db.add(\n",
    "#     embeddings = [embedding_1],\n",
    "#     documents = [student_info],\n",
    "#     ids = [\"id1\"]\n",
    "# )\n",
    "\n",
    "\n",
    "# collection = chroma_client.get_or_create_collection(name=\"resume_vector\")\n",
    "# col_exists = True\n",
    "# try:\n",
    "#     coll =  .get_collection(name=\"resume_vector\")\n",
    "# except:\n",
    "#     col_exists = False\n",
    "\n",
    "\n",
    "print(collection_of_resumes_db.count())\n",
    "for index, row in df1.iterrows():\n",
    "    collection_of_resumes_db.add(\n",
    "        embeddings = [row[\"embeddings\"]],\n",
    "        documents = [row[\"Resume_str\"]],\n",
    "        ids = [str(row[\"index\"])]\n",
    "    )\n",
    "print(collection_of_resumes_db.count())\n",
    "\n",
    "# resume_latest_data_df.apply(lambda x: add_embedding_to_db(collection_of_resumes_db, x.embeddings, x.Resume_str, str(x.index)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "# for index, row in resume_latest_data_df.iterrows():\n",
    "#     if row[\"index\"]==10 or row[\"index\"]==11:\n",
    "#         print(row['Resume_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15844"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXTRA\n",
    "# collection_of_resumes_db.peek()\n",
    "\n",
    "# all_documents = collection_of_resumes_db.get()['documents']\n",
    "# len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Does any candidate already has Python coding experience? \n",
    "Please summarise their experience and provide their resume id. \n",
    "Also provide ids of other matching resumes\"\"\"\n",
    "\n",
    "questions_embedding = get_embedding(azure_client, question)\n",
    "\n",
    "results = collection_of_resumes_db.query(\n",
    "    query_embeddings=[questions_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "best_result_document = results.get('documents')[0][0]\n",
    "\n",
    "best_result_index = int(results.get('ids')[0][0])\n",
    "best_result_id = resume_latest_data_df.loc[(resume_latest_data_df.index==best_result_index)]['ID']\n",
    "best_result_id = int(best_result_id.iloc[0])\n",
    "\n",
    "list_of_resume_ids = []\n",
    "for i in range(1,5):\n",
    "    other_result_index = int(results.get('ids')[0][i])\n",
    "    other_result_id = resume_latest_data_df.loc[(resume_latest_data_df.index==other_result_index)]['ID']\n",
    "    list_of_resume_ids.append((other_result_id.iloc[0]))\n",
    "\n",
    "string_of_resume_ids = ','.join(map(str, list_of_resume_ids))\n",
    "best_result = best_result_document + f\". The id of this specific resume is {best_result_id}. Also, other matching resumes are {string_of_resume_ids}\"\n",
    "print(best_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the candidate with the resume ID 50328713 has Python coding experience. Here's a summary of their experience:\n",
      "\n",
      "- The candidate has listed Python as one of their skills, indicating they are conversant with the language.\n",
      "- They have experience with machine learning tools and libraries such as Scikit-learn, Pandas, Seaborn, matplotlib, and basic working knowledge of TensorFlow.\n",
      "- They built a machine learning model using the XGBoost algorithm that achieved a 77.5% accuracy rate in the Kaggle Titanic challenge. This demonstrates practical application of their Python skills in a project setting.\n",
      "\n",
      "The resume ID for this candidate is 50328713.\n",
      "\n",
      "The other matching resumes you asked for are:\n",
      "\n",
      "- 32985311\n",
      "- 12011623\n",
      "- 14871762\n",
      "- 11813872\n",
      "\n",
      "Please note these ids are given without context or content; whether they have Python experience is not determinable from the information provided here. If their resumes also list Python as a skill or describe Python-based projects, those candidates would have Python coding experience. However, since the content of these resumes isn't provided, it's not possible to summarize their experience without reviewing the actual resumes.\n"
     ]
    }
   ],
   "source": [
    "persona = \"\"\"\n",
    "You are expert in reviewing resumes and identifying the best candidates for a role. You are an advisor to the HR team, especially in answering any queries\n",
    "\"\"\"\n",
    "persona=persona+best_result\n",
    "response = answer_question(client=azure_client, question=question, persona=persona)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Re-arrange the code and the function order\n",
    "- Update the name of the columns in the dataframe\n",
    "- Show both approaches ie with and without vector DB (without DB shows more mathematics)\n",
    "- Decide what to do with csvs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45ca95aefd86e0e372b2a57f2bf5af2dad170a3cafdf2ca91f8dc558701f3c9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
